# -*- coding: utf-8 -*-
"""fabric_pattern.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gOOiucnR8MXPMSqbcEdslOdNPSieZwtu
"""

# Commented out IPython magic to ensure Python compatibility.
# !pip install scikit-image
# !pip install google-colab

from google.colab.patches import cv2_imshow
import cv2
import numpy as np

from skimage.feature import hog, local_binary_pattern
from skimage import exposure
from scipy.spatial.distance import correlation

from skimage.metrics import structural_similarity as ssim, hausdorff_distance, normalized_mutual_information
from skimage.util import compare_images

import matplotlib.pyplot as plt
# %matplotlib inline

!pip install keras.ops

# !pip list

from tensorflow.python.framework import ops as tf_ops

!pip install keras
!pip install tensorflow

tf.__version__

!python
import keras
# keras.__version__

keras.__version__

!pip install tensorflow-gpu==1.9.0

!pip install keras==2.2.0

from warnings import filterwarnings
filterwarnings('ignore')
import matplotlib.pyplot as plt
import numpy as np
import os
import glob
import random
import tensorflow as tf
from pathlib import Path
from keras import applications
from keras import layers
from keras import losses
from tensorflow.python.framework import ops as tf_ops
# from keras import ops
from keras import optimizers
from keras import metrics
from keras import Model
from keras.applications import resnet
from tensorflow.keras import layers, Model
from tensorflow.keras.callbacks import EarlyStopping
from keras.preprocessing.image import ImageDataGenerator
import tensorflow.keras.backend as k

# Set the target shape
target_shape = (200, 200)

# Define the cache directory
cache_dir = Path("/content/cache_data")

# Ensure the cache directory exists
cache_dir.mkdir(parents=True, exist_ok=True)

# Download and extract the zip files
!gdown --id 1jvkbTr_giSP3Ru8OwGNCg6B4PvVbcO34 -O /content/test.zip
!gdown --id 1EzBZUb_mh_Dp_FKD0P4XiYYSd0QBH5zW -O /content/right.zip
!unzip -oq /content/test.zip -d /content/cache_data/test
!unzip -oq /content/right.zip -d /content/cache_data/right

# Function to preprocess images
def preprocess_img(filename):
    """Load the specified file as a JPEG image, preprocess it, and resize it to the target shape."""
    image_string = tf.io.read_file(filename)
    image = tf.image.decode_jpeg(image_string, channels=3)
    image = tf.image.convert_image_dtype(image, tf.float32)
    image = tf.image.resize(image, target_shape)
    return image

# Function to preprocess triplets
def preprocess_triplets(anchor, positive, negative):
    """Given the filenames corresponding to the three images, load and preprocess them."""
    return (
        preprocess_img(anchor),
        preprocess_img(positive),
        preprocess_img(negative)
    )

# Get the list of anchor, positive, and negative images
anchor_images_path = cache_dir / "test"
positive_images_path = cache_dir / "right"

anchor_images = sorted([str(anchor_images_path / f) for f in os.listdir(anchor_images_path)])
positive_images = sorted([str(positive_images_path / f) for f in os.listdir(positive_images_path)])
# positive_images = sorted(glob.glob(str(positive_images_path / "/content/new15.jpg")))

image_count = len(anchor_images)


# Create datasets
anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)
positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)

# Randomize the list of available images for negative dataset
rng = np.random.RandomState(seed=42)
rng.shuffle(anchor_images)
rng.shuffle(positive_images)

negative_images = anchor_images + positive_images
np.random.RandomState(seed=32).shuffle(negative_images)

negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)
negative_dataset = negative_dataset.shuffle(buffer_size=4096)

# Create the combined dataset
dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))
dataset = dataset.shuffle(buffer_size=1024)
dataset = dataset.map(preprocess_triplets)

# Split the dataset into train and validation sets
train_dataset = dataset.take(round(image_count * 0.8))
val_dataset = dataset.skip(round(image_count * 0.8))

# Batch and prefetch the datasets
train_dataset = train_dataset.batch(32, drop_remainder=False)
train_dataset = train_dataset.prefetch(tf.data.AUTOTUNE)

val_dataset = val_dataset.batch(32, drop_remainder=False)
val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)


# Visualize a few triplets from the dataset
# def visualize(anchor, positive, negative):
#     """Visualize a few triplets from the supplied batches."""
#     plt.figure(figsize=(9, 9))
#     for i in range(3):
#         plt.subplot(3, 3, i*3 + 1)
#         plt.imshow(anchor[i])
#         plt.axis('off')
#         plt.title('Anchor')

#         plt.subplot(3, 3, i*3 + 2)
#         plt.imshow(positive[i])
#         plt.axis('off')
#         plt.title('Positive')

#         plt.subplot(3, 3, i*3 + 3)
#         plt.imshow(negative[i])
#         plt.axis('off')
#         plt.title('Negative')
#     plt.tight_layout()
#     plt.show()

# # Visualize a few triplets from the training dataset
# visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])


base_cnn = resnet.ResNet50(
    weights="imagenet", input_shape=target_shape + (3,), include_top=False
)

flatten = layers.Flatten()(base_cnn.output)
dense1 = layers.Dense(512, activation="relu")(flatten)
dense1 = layers.BatchNormalization()(dense1)
dense2 = layers.Dense(256, activation="relu")(dense1)
dense2 = layers.BatchNormalization()(dense2)
output = layers.Dense(256)(dense2)

embedding = Model(base_cnn.input, output, name="Embedding")

trainable = False
for layer in base_cnn.layers:
    if layer.name == "conv5_block1_out":
        trainable = True
    layer.trainable = trainable

class DistanceLayer(layers.Layer):
    """
    This layer is responsible for computing the distance between the anchor
    embedding and the positive embedding, and the anchor embedding and the
    negative embedding.
    """

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    def call(self, anchor, positive, negative):
        # ap_distance = ops.sum(tf.square(anchor - positive), -1)
        # an_distance = ops.sum(tf.square(anchor - negative), -1)
        # return (ap_distance, an_distance)
        ap_distance = k.sum(tf.square(anchor - positive), -1,keepdims=True)
        an_distance = k.sum(tf.square(anchor - negative), -1,keepdims=True)
        return (ap_distance, an_distance)


anchor_input = layers.Input(name="anchor", shape=target_shape + (3,))
positive_input = layers.Input(name="positive", shape=target_shape + (3,))
negative_input = layers.Input(name="negative", shape=target_shape + (3,))

distances = DistanceLayer()(
    embedding(resnet.preprocess_input(anchor_input)),
    embedding(resnet.preprocess_input(positive_input)),
    embedding(resnet.preprocess_input(negative_input)),
)

siamese_network = Model(
    inputs=[anchor_input, positive_input, negative_input], outputs=distances
)



class SiameseModel(Model):
    """The Siamese Network model with a custom training and testing loops.

    Computes the triplet loss using the three embeddings produced by the
    Siamese Network.

    The triplet loss is defined as:
       L(A, P, N) = max(‖f(A) - f(P)‖² - ‖f(A) - f(N)‖² + margin, 0)
    """

    def __init__(self, siamese_network, margin=0.5):
        super().__init__()
        self.siamese_network = siamese_network
        self.margin = margin
        self.loss_tracker = metrics.Mean(name="loss")

    def call(self, inputs):
        return self.siamese_network(inputs)

    def train_step(self, data):
        # GradientTape is a context manager that records every operation that
        # you do inside. We are using it here to compute the loss so we can get
        # the gradients and apply them using the optimizer specified in
        # `compile()`.
        with tf.GradientTape() as tape:
            loss = self._compute_loss(data)

        # Storing the gradients of the loss function with respect to the
        # weights/parameters.
        gradients = tape.gradient(loss, self.siamese_network.trainable_weights)

        # Applying the gradients on the model using the specified optimizer
        self.optimizer.apply_gradients(
            zip(gradients, self.siamese_network.trainable_weights)
        )

        # Let's update and return the training loss metric.
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

    def test_step(self, data):
        loss = self._compute_loss(data)

        # Let's update and return the loss metric.
        self.loss_tracker.update_state(loss)
        return {"loss": self.loss_tracker.result()}

    def _compute_loss(self, data):
        # The output of the network is a tuple containing the distances
        # between the anchor and the positive example, and the anchor and
        # the negative example.
        ap_distance, an_distance = self.siamese_network(data)

        # Computing the Triplet Loss by subtracting both distances and
        # making sure we don't get a negative value.
        loss = ap_distance - an_distance
        loss = tf.maximum(loss + self.margin, 0.0)
        return loss

    @property
    def metrics(self):
        # We need to list our metrics here so the `reset_states()` can be
        # called automatically.
        return [self.loss_tracker]


# Now you can use train_dataset and val_dataset for training your model
siamese_model = SiameseModel(siamese_network)
# Define the number of validation steps
validation_steps = round((1 - 0.8) * image_count) // 32  # Assuming batch size of 32
siamese_model.compile(optimizer=optimizers.Adam(0.0001), metrics=['accuracy'])
# Train the model using the train_dataset and specify the validation_steps
siamese_model.fit(train_dataset, epochs=10, validation_data=val_dataset, validation_steps=validation_steps)


# siamese_model.fit(train_dataset, val_dataset, epochs=10, validation_data=val_dataset)





# import tensorflow as tf
# import os
# import glob

# # Define the paths to your extracted image directories
# anchor_images_dir = '/content/test.zip'  # Path to the directory containing anchor images
# positive_images_dir = '/content/right.zip'   # Path to the directory containing positive images

# # List all image filenames in the directories
# anchor_images = [os.path.join(anchor_images_dir, fname) for fname in os.listdir(anchor_images_dir)]
# positive_images = [os.path.join(positive_images_dir, fname) for fname in os.listdir(positive_images_dir)]

# # Shuffle the images (optional but recommended)
# random.shuffle(anchor_images)
# random.shuffle(positive_images)

# # Combine anchor and positive images to create negative pairs
# negative_images = anchor_images + positive_images
# random.shuffle(negative_images)

# # Create TensorFlow datasets from image paths
# anchor_dataset = tf.data.Dataset.from_tensor_slices(anchor_images)
# positive_dataset = tf.data.Dataset.from_tensor_slices(positive_images)
# negative_dataset = tf.data.Dataset.from_tensor_slices(negative_images)

# # Define preprocessing functions
# def preprocess_img(filename):
#     image_string = tf.io.read_file(filename)
#     image = tf.image.decode_jpeg(image_string, channels=3)
#     image = tf.image.convert_image_dtype(image, tf.float32)
#     image = tf.image.resize(image, target_shape)  # Ensure the image is resized to the target shape
#     return image

# def preprocess_triplets(anchor, positive, negative):
#     return (
#         preprocess_img(anchor),
#         preprocess_img(positive),
#         preprocess_img(negative)
#     )

# # Zip the datasets and apply preprocessing
# dataset = tf.data.Dataset.zip((anchor_dataset, positive_dataset, negative_dataset))
# dataset = dataset.shuffle(buffer_size=1024)  # Shuffle the dataset
# dataset = dataset.map(preprocess_triplets)

# # Split the dataset into training and validation sets
# train_size = int(0.8 * len(anchor_images))  # 80% for training, 20% for validation
# train_dataset = dataset.take(train_size)
# val_dataset = dataset.skip(train_size)

# # Batch and prefetch the datasets
# batch_size = 32
# train_dataset = train_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)
# val_dataset = val_dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)

# # # Visualize a few triplets from the dataset
# # def visualize(anchor, positive, negative):
# #     """Visualize a few triplets from the supplied batches."""
# #     plt.figure(figsize=(9, 9))
# #     for i in range(3):
# #         plt.subplot(3, 3, i*3 + 1)
# #         plt.imshow(anchor[i])
# #         plt.axis('off')
# #         plt.title('Anchor')

# #         plt.subplot(3, 3, i*3 + 2)
# #         plt.imshow(positive[i])
# #         plt.axis('off')
# #         plt.title('Positive')

# #         plt.subplot(3, 3, i*3 + 3)
# #         plt.imshow(negative[i])
# #         plt.axis('off')
# #         plt.title('Negative')
# #     plt.tight_layout()
# #     plt.show()

# # # Visualize a few triplets from the training dataset
# # visualize(*list(train_dataset.take(1).as_numpy_iterator())[0])

# class DistanceLayer(layers.Layer):
#     """
#     This layer is responsible for computing the distance between the anchor
#     embedding and the positive embedding, and the anchor embedding and the
#     negative embedding.
#     """

#     def __init__(self, **kwargs):
#         super().__init__(**kwargs)

#     def call(self, anchor, positive, negative):
#         ap_distance = ops.sum(tf.square(anchor - positive), -1)
#         an_distance = ops.sum(tf.square(anchor - negative), -1)
#         return (ap_distance, an_distance)


# anchor_input = layers.Input(name="anchor", shape=target_shape + (3,))
# positive_input = layers.Input(name="positive", shape=target_shape + (3,))
# negative_input = layers.Input(name="negative", shape=target_shape + (3,))

# distances = DistanceLayer()(
#     embedding(resnet.preprocess_input(anchor_input)),
#     embedding(resnet.preprocess_input(positive_input)),
#     embedding(resnet.preprocess_input(negative_input)),
# )

# siamese_network = Model(
#     inputs=[anchor_input, positive_input, negative_input], outputs=distances
# )







"""# method 1"""

original_fabric1 = cv2.imread('/content/102.jpg')
height, width, color = original_fabric1.shape

_, design_pattern1 = hog(original_fabric1, orientations=20,
                         pixels_per_cell=(8,8),cells_per_block=(2,2),
                         visualize = True, multichannel= True)

cv2_imshow(original_fabric1)
cv2_imshow(design_pattern1)
cv2.waitKey(0)
cv2.destroyAllWindows()
plt.tight_layout()

original_fabric2 = cv2.imread('/content/15.jpg')
height, width, color = original_fabric2.shape

_, design_pattern2 = hog(original_fabric2, orientations=20,
                         pixels_per_cell=(8,8),cells_per_block=(2,2),
                         visualize = True, multichannel= True)

cv2_imshow(original_fabric2)
cv2_imshow(design_pattern2)
cv2.waitKey(0)
cv2.destroyAllWindows()
plt.tight_layout()



test_fabric = cv2.imread('/content/64.jpg')
height, width, color = test_fabric.shape

_, test_pattern = hog(test_fabric, orientations=20,
                         pixels_per_cell=(8,8),cells_per_block=(2,2),
                         visualize = True, multichannel= True)

cv2_imshow(test_fabric)
cv2_imshow(test_pattern)
cv2.waitKey(0)
cv2.destroyAllWindows()
plt.tight_layout()

# normalize the values of pattern we have to compaer the pattern with the
#  help of correlation and the corr takes the  on 1d vaue
# rescale the output and the normalize it

design_pattern1 = exposure.rescale_intensity(design_pattern1, in_range=(0,10))
design_pattern2 = exposure.rescale_intensity(design_pattern2, in_range=(0,10))
test_pattern = exposure.rescale_intensity(test_pattern, in_range=(0,10))

#flatten the values
orignal_pattern1 = design_pattern1.flatten()
orignal_pattern2 = design_pattern2.flatten()
testing_pattern = test_pattern.flatten()

orignal_pattern1

orignal_pattern2

testing_pattern

if orignal_pattern2.shape[0] != testing_pattern.shape[0]:
    # reshape the smaller array to match the larger one
    if orignal_pattern2.shape[0] < testing_pattern.shape[0]:
        orignal_pattern2 = np.repeat(orignal_pattern2, testing_pattern.shape[0] // orignal_pattern2.shape[0])
    else:
        testing_pattern = np.repeat(testing_pattern, orignal_pattern2.shape[0] // testing_pattern.shape[0])

# 3. **Rerun the correlation function**:

#display results
#  display
plt.figure(figsize=[20,10]);
plt.subplot(2,3,1);
plt.axis('off');
plt.imshow(test_pattern);
plt.title('test_pattern_64')

plt.subplot(2,3,2);
plt.axis('off');
plt.imshow(design_pattern1);
plt.title('design_pattern1_102')

plt.subplot(2,3,3);
plt.axis('off');
plt.imshow(design_pattern2);
plt.title('design_patern_15')

plt.tight_layout()

# comparing the both the output
correlation(orignal_pattern1,testing_pattern)

# comparing the both the output
correlation(orignal_pattern2,testing_pattern)

# comparing the both the output
correlation(orignal_pattern2,orignal_pattern1)

import scipy.spatial.distance as ssd

print(orignal_pattern1.shape)
print(testing_pattern.shape)

from scipy.spatial.distance import cosine
correlation = 1 - cosine(orignal_pattern1, testing_pattern)

correlation = ssd.correlation(orignal_pattern1, testing_pattern)
correlation







# fabric_img = cv2.imread('/content/20240406_160744.mp4_frame_0057.jpg', cv2.IMREAD_GRAYSCALE)
# height, weight = fabric_image.shape

# total_points = 16
# object_radius = 8
# fabric_texture = local_binary_pattern(fabric_image,16,8,method='uniform' )

# total_binx = int(fabric_texture.max()+1)
# texture_data,_ = np.histogram(fabric_texture,density=True,bins=total_binx,range=(0,total_bins))
# fig,axes = plt.subplot(1,3,figsize=(width/50,height/50))
# axes[0].set_title("orignal_fabri")
# axes[0].imshow(fabric_image)

# axes[1].set_title("grayscale_fabri")
# axes[1].imshow(fabric_image,cmap="gray")

# axes[2].set_title("fabric_texteure")
# axes[2].imshow(fabric_texture,cmap='gray')

# plt.tight_layout()

fabric_image1 = cv2.imread('/content/64.jpg', cv2.IMREAD_GRAYSCALE)
height, weight = fabric_image1.shape

total_points = 16
object_radius = 8
fabric_texture1 = local_binary_pattern(fabric_image1, 16, 8, method='uniform')

total_bins = int(fabric_texture1.max() + 1)
texture_data, _ = np.histogram(fabric_texture1, density=True, bins=total_bins, range=(0, total_bins))

# Create a figure with three subplots
fig, axes = plt.subplots(1, 3, figsize=(width / 50, height / 50))

# Set the titles and images for each subplot
axes[0].set_title("Original Fabric")
axes[0].imshow(fabric_image1)

axes[1].set_title("Grayscale Fabric")
axes[1].imshow(fabric_image1)

axes[2].set_title(" Fabric")
axes[2].imshow(fabric_texture1)
plt.tight_layout()

fabric_image2 = cv2.imread('/content/15.jpg', cv2.IMREAD_GRAYSCALE)
height, weight = fabric_image2.shape

total_points = 16
object_radius = 8
fabric_texture2 = local_binary_pattern(fabric_image2, 16, 8, method='uniform')

total_bins = int(fabric_texture2.max() + 1)
texture_data, _ = np.histogram(fabric_texture2, density=True, bins=total_bins, range=(0, total_bins))

# Create a figure with three subplots
fig, axes = plt.subplots(1, 3, figsize=(width / 50, height / 50))

# Set the titles and images for each subplot
axes[0].set_title("Original Fabric")
axes[0].imshow(fabric_image2)

axes[1].set_title("Grayscale Fabric")
axes[1].imshow(fabric_image2)

axes[2].set_title(" Fabric")
axes[2].imshow(fabric_texture2)

plt.tight_layout()

fabric_image3 = cv2.imread('/content/102.jpg', cv2.IMREAD_GRAYSCALE)
height, weight = fabric_image3.shape

total_points = 16
object_radius = 8
fabric_texture3 = local_binary_pattern(fabric_image3, 16, 8, method='uniform')

total_bins = int(fabric_texture3.max() + 1)
texture_data, _ = np.histogram(fabric_texture3, density=True, bins=total_bins, range=(0, total_bins))

# Create a figure with three subplots
fig, axes = plt.subplots(1, 3, figsize=(width / 50, height / 50))

# Set the titles and images for each subplot
axes[0].set_title("Original Fabric")
axes[0].imshow(fabric_image3)

axes[1].set_title("Grayscale Fabric")
axes[1].imshow(fabric_image3)

axes[2].set_title(" Fabric")
axes[2].imshow(fabric_texture3)

plt.tight_layout()

design_pattern2 = exposure.rescale_intensity(fabric_texture1, in_range=(0,10))
design_pattern3 = exposure.rescale_intensity(fabric_texture3, in_range=(0,10))
test_pattern = exposure.rescale_intensity(fabric_texture2, in_range=(0,10))

#flatten the values
orignal_pattern2 = design_pattern2.flatten()
orignal_pattern3 = design_pattern3.flatten()
testing_pattern = test_pattern.flatten()
# comparing the both the output
# print('15 and 64: = ',correlation(orignal_pattern2,testing_pattern))
# print('15 and 102: = ',correlation(orignal_pattern3,testing_pattern))
# print('both orignal: = ',correlation(orignal_pattern2,orignal_pattern3))
# print('15and102: = ',correlation(orignal_pattern3,testing_pattern))

correlation1 = np.corrcoef(orignal_pattern2, testing_pattern)[0, 1]
print('corr1:' ,correlation1)
correlation2 = np.corrcoef(orignal_pattern1, testing_pattern)[0, 1]
print('corr2:',correlation2)
correlation3 = np.corrcoef(orignal_pattern1, orignal_pattern2)[0, 1]
print('corr3:',correlation3)

#display results
#  display
plt.figure(figsize=[20,10]);
plt.subplot(2,3,1);
plt.axis('off');
plt.imshow(fabric_texture1);
plt.title('fabric_64')

plt.subplot(2,3,2);
plt.axis('off');
plt.imshow(fabric_texture2);
plt.title('fabric_15')

plt.subplot(2,3,3);
plt.axis('off');
plt.imshow(fabric_texture3);
plt.title('fabric_102')

plt.tight_layout()

999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999999





"""# feature and aligment"""

## Reading the template and scannedd img
import cv2
import numpy as np
import matplotlib.pyplot as plt

#read refernect img
reffilename = '/content/64.jpg'
print("reading refernece img: ", reffilename)
im1 = cv2.imread(reffilename , cv2.IMREAD_COLOR)
im1= cv2.cvtColor(im1, cv2.COLOR_BGR2RGB)

# read img to be aligned
imffilename = "/content/15.jpg"
print("reading refernece img: ", imffilename)
im2 = cv2.imread(imffilename , cv2.IMREAD_COLOR)
im2= cv2.cvtColor(im2, cv2.COLOR_BGR2RGB)

#  display
plt.figure(figsize=[20,10]);
plt.subplot(121);
plt.axis('off');
plt.imshow(im1);
plt.title('orignal')

plt.subplot(122);
plt.axis('off');
plt.imshow(im2);
plt.title('scanned')
plt.tight_layout()

## find key point s in both img

#conver img to grayscale
im1_gray = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)
im2_gray = cv2.cvtColor(im2, cv2.COLOR_BGR2GRAY)

#detection of ORB features and compute descriptions
# vector representatiom of the pixel information around thekey points and
# to amtch the keypoint we have used decriptor to match them up
# centre of the circle is the loacion of the keypoint,the size of te circle
# represent  the sacle of the key pointand the line connnecting the centre and the
# circle represent the orientaton of the keypoint

MAX_NUM_FEATURE = 1500
orb = cv2.ORB_create(MAX_NUM_FEATURE)
keypoints1, description1 = orb.detectAndCompute(im1_gray, None)
keypoints2, description2 = orb.detectAndCompute(im2_gray, None)

# display
im1_display = cv2.drawKeypoints(im1, keypoints1,
                                outImage=np.array([]), color=(255,0,0),
                                flags= cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

im2_display = cv2.drawKeypoints(im2, keypoints2,
                                outImage=np.array([]), color=(255,0,0),
                                flags= cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)

#  display
plt.figure(figsize=[20,10]);
plt.subplot(121);
plt.axis('off');
plt.imshow(im1_display);
plt.title('orignal')

plt.subplot(122);
plt.axis('off');
plt.imshow(im2_display);
plt.title('scanned')

plt.tight_layout()

## matching the points

matcher = cv2.DescriptorMatcher_create(cv2.DESCRIPTOR_MATCHER_BRUTEFORCE_HAMMING)
matches = matcher.match(description1, description2, None)

#sort matches by score
# matches.sort(key=lambda x: x.distance, reverse=False)
# matches = sorted(matcher.match(description1, description2, None), key=lambda x: x.distance, reverse=False)

matches = list(matcher.match(description1, description2, None))
matches.sort(key=lambda x: x.distance, reverse=False)
matches = sorted(matcher.match(description1, description2, None), key=lambda x: x.distance, reverse=False)

#remove not so good matcher
numGoodmatchers = int(len(matches)*0.1)
matches = matches[:numGoodmatchers]

#draw top matches
im_matches = cv2.drawMatches(im1, keypoints1, im2, keypoints2, matches, None)

plt.figure(figsize=[40,10])
plt.imshow(im_matches);
plt.axis('off');
plt.title('orignal form');
plt.tight_layout()

#find homography
#extract location of good matche
points1 = np.zeros((len(matches),2),dtype = np.float32)

points2 = np.zeros((len(matches),2),dtype = np.float32)

for i, match in enumerate(matches):
  points1[i,:]= keypoints1[match.queryIdx].pt
  points2[i,:]= keypoints2[match.trainIdx].pt

#find homography
h, mask = cv2.findHomography(points2,points1,cv2.RANSAC)  #to use very robust to filter out outliers left over from ,

#warp image
#use homography to warp image
height , width, channels = im1.shape
im2_reg = cv2.warpPerspective(im2,h, (width, height))

#display results
#  display
plt.figure(figsize=[20,10]);
plt.subplot(2,3,1);
plt.axis('off');
plt.imshow(im1);
plt.title('orignal')

plt.subplot(2,3,2);
plt.axis('off');
plt.imshow(im2_reg);
plt.title('scanned')

plt.subplot(2,3,3);
plt.axis('off');
plt.imshow(h);
plt.title('scanned')

plt.subplot(2,3,4);
plt.axis('off');
plt.imshow(mask);
plt.title('scanned')

plt.tight_layout()

design_pattern2 = exposure.rescale_intensity(im1, in_range=(0,10))
test_pattern = exposure.rescale_intensity(im2, in_range=(0,10))

#flatten the values
orignal_pattern2 = design_pattern2.flatten()
testing_pattern = test_pattern.flatten()

# comparing the both the output
# correlation(orignal_pattern2,testing_pattern)

correlation1 = np.corrcoef(orignal_pattern2, testing_pattern)[0, 1]
print('corr1:' ,correlation1)
# correlation2 = np.corrcoef(orignal_pattern1, testing_pattern)[0, 1]
# print('corr2:',correlation2)
# correlation3 = np.corrcoef(orignal_pattern1, orignal_pattern2)[0, 1]
# print('corr3:',correlation3)







import cv2
import numpy as np
import matplotlib.pyplot as plt

# Load original image and test image
original_img = cv2.imread("/content/15.jpg", 0)
test_img = cv2.imread("/content/64.jpg", 0)

# Apply pattern recognition algorithm (for example, correlation coefficient)
correlation_coefficient = cv2.matchTemplate(original_img, test_img, cv2.TM_CCOEFF_NORMED)

# Display correlation coefficient
print("Correlation Coefficient:", correlation_coefficient[0][0])

# Display original image pattern data
plt.subplot(1, 2, 1)
plt.imshow(original_img, cmap='gray')
plt.title('Original Pattern')

# Display test image pattern data
plt.subplot(1, 2, 2)
plt.imshow(test_img, cmap='gray')
plt.title('Test Pattern')

plt.show()
plt.tight_layout()





"""This code performs several image comparison and pattern recognition tasks using various metrics and techniques. Let's break down the code and explain each part:

1. **Importing Libraries:** Import necessary libraries such as OpenCV, scikit-image, and matplotlib for image processing and visualization.Import Libraries:

cv2: OpenCV library for computer vision tasks.
structural_similarity, hausdorff_distance, normalized_mutual_information, compare_images: Functions from skimage library for image comparison and similarity metrics.
cv2_imshow: Function from Google Colab to display images.

2. **Loading Images:** Load the original image and the test image from file paths. Convert them to grayscale using `cv2.IMREAD_GRAYSCALE`.

Compute Similarity Metrics:

Computes various similarity metrics between the original and test images, such as SSIM (Structural Similarity Index), Hausdorff Distance, Normalized Mutual Information, and Correlation Coefficient using the respective functions.

3. **SSIM Calculation:** Calculate the Structural Similarity Index (SSIM) between the original and test images using the `structural_similarity` function from scikit-image. This metric measures the similarity between two images.

4. **Hausdorff Distance Calculation:** Calculate the Hausdorff distance between the original and test images using the `hausdorff_distance` function from scikit-image. Hausdorff distance measures the maximum distance of a set to the nearest point in the other set.

5. **Normalized Mutual Information Calculation:** Calculate the Normalized Mutual Information between the original and test images using the `normalized_mutual_information` function from scikit-image. Mutual Information measures the amount of information obtained about one variable through another variable.

Image Comparison:

Compares images using different methods like absolute difference, blend, and checkerboard using the compare_images() function from skimage.
Computes local binary patterns (LBP) for both images using the local_binary_pattern() function from skimage.
Rescales the intensity of the LBP images using exposure.rescale_intensity() from skimage.
Flattens the intensity values of the images.
Computes the correlation coefficient between the intensity values of the original and test images using the correlation() function.

6. **Correlation Coefficient Calculation:** Calculate the correlation coefficient between the original and test images using the `matchTemplate` function from OpenCV. This measures how well the test image matches a given template (in this case, the original image).

7. **Image Comparison Using skimage:** Use the `compare_images` function from scikit-image to compare the original and test images using different methods such as 'diff', 'blend', and 'checkerboard'. This function returns a coefficient representing the similarity between the images.

8. **Local Binary Pattern (LBP) Feature Extraction:** Extract LBP features from both the original and test images using the `local_binary_pattern` function from scikit-image. LBP is a texture descriptor used for texture classification and segmentation.

9. **Rescaling Intensity:** Rescale the intensity of LBP features to a specific range using the `rescale_intensity` function from scikit-image.

10. **Correlation Calculation:** Calculate the correlation between the LBP features of the original and test images using the `correlation` function. This measures the similarity of the texture patterns.

11. **Displaying Results:** Display the calculated metrics and comparison results, including SSIM score, Hausdorff distance, Normalized Mutual Information, Correlation Coefficient, and comparison coefficients using `print` statements. Also, visualize the images and comparison results using matplotlib's `imshow` function.Releases OpenCV windows using cv2.destroyAllWindows().
Adjusts the layout of the plots using plt.tight_layout().

Overall, this code provides a comprehensive analysis of image similarity and pattern recognition using various metrics and techniques, making it suitable for evaluating fabric patterns or other visual similarities in images.

The Structural Similarity Index (SSIM) score measures the similarity between two images. It ranges from -1 to 1, where:

1 indicates that the images are identical.
0 indicates no similarity between the images.
-1 indicates perfect dissimilarity between the images.



The value of Normalized Mutual Information (NMI) can range from 0 to 1.

In the context of image processing, NMI measures the mutual information shared between two images, normalized to the range [0, 1]. A value of 1 indicates perfect agreement or similarity between the images, while a value of 0 indicates no mutual information or complete dissimilarity.



The Hausdorff Distance is a measure of the "closeness" between two sets of points in a metric space. In the context of image processing, it's commonly used to quantify the similarity between two images or shapes.

A Hausdorff Distance of 2.23606797749979 indicates the "distance" or dissimilarity between two sets of points. However, it's important to note that the interpretation of this value depends on the context and the specific application.

In general, a lower Hausdorff Distance indicates a higher degree of similarity between the sets of points, while a higher distance suggests greater dissimilarity.
"""

import cv2
from skimage.metrics import structural_similarity as ssim, hausdorff_distance, normalized_mutual_information
from skimage.util import compare_images
from google.colab.patches import cv2_imshow


# Load original image and test image
original_img = cv2.imread("/content/abc/15.jpg",  cv2.IMREAD_GRAYSCALE)
test_img = cv2.imread("/content/64.jpg",  cv2.IMREAD_GRAYSCALE)

# Compute SSIM between original and test images
total_bins = int(original_img.max() + 1)
ssim_score, diff_img = ssim(original_img, test_img, full=True, range=(0, total_bins),channel_axis=-1, multichannel=True)

hau = hausdorff_distance(original_img, test_img)

nor = normalized_mutual_information(original_img, test_img, bins=100)

correlation_coefficient = cv2.matchTemplate(original_img, test_img, cv2.TM_CCOEFF_NORMED)

compare1 = compare_images(original_img, test_img, method='diff', n_tiles=(8, 8))
compare2 = compare_images(original_img, test_img, method='blend', n_tiles=(8, 8))
compare3 = compare_images(original_img, test_img, method='checkerboard', n_tiles=(8, 8))
# {‘diff’, ‘blend’, ‘checkerboard’}.'diff' computes the absolute difference between the two images.
# 'blend' computes the mean value. 'checkerboard' makes tiles of dimension n_tiles that display alternatively the first and the second image.

# fabric_image1 = cv2.imread(original_img, cv2.IMREAD_GRAYSCALE)
height, weight = original_img.shape

total_points = 16
object_radius = 8
fabric_texture1 = local_binary_pattern(original_img, 16, 8, method='uniform')

total_bins = int(fabric_texture1.max() + 1)
texture_data, _ = np.histogram(fabric_texture1, density=True, bins=total_bins, range=(0, total_bins))

# Create a figure with three subplots
fig, axes = plt.subplots(1, 3, figsize=(width / 50, height / 50))

# fabric_image2 = cv2.imread(test_img, cv2.IMREAD_GRAYSCALE)
height, weight = test_img.shape

total_points = 16
object_radius = 8
fabric_texture2 = local_binary_pattern(test_img, 16, 8, method='uniform')

total_bins = int(fabric_texture2.max() + 1)
texture_data, _ = np.histogram(fabric_texture2, density=True, bins=total_bins, range=(0, total_bins))

# Create a figure with three subplots
fig, axes = plt.subplots(1, 3, figsize=(width / 50, height / 50))

design_pattern2 = exposure.rescale_intensity(fabric_texture1, in_range=(0,10))
# design_pattern3 = exposure.rescale_intensity(fabric_texture3, in_range=(0,10))
test_pattern = exposure.rescale_intensity(fabric_texture2, in_range=(0,10))

#flatten the values
orignal_pattern2 = design_pattern2.flatten()
# orignal_pattern3 = design_pattern3.flatten()
testing_pattern = test_pattern.flatten()
# comparing the both the output
# print('56 and 57: = ',correlation(orignal_pattern2,testing_pattern))
# print('15 and 102: = ',correlation(orignal_pattern3,testing_pattern))
# print('both orignal: = ',correlation(orignal_pattern2,orignal_pattern3))

correlation1 = np.corrcoef(orignal_pattern2, testing_pattern)[0, 1]
print('corr1:' ,correlation1)
# correlation2 = np.corrcoef(orignal_pattern1, testing_pattern)[0, 1]
# print('corr2:',correlation2)
# correlation3 = np.corrcoef(orignal_pattern1, orignal_pattern2)[0, 1]
# print('corr3:',correlation3)


# Display SSIM score
print("Hausdorff Distance:", hau)
print("Normalized Mutual Information:", nor)

# Maximum possible value of NMI (which occurs when the two images are identical)
max_nmi_value = 1.0

# Corrected NMI value
corrected_nmi_value = min(nor / max_nmi_value, 1.0)

print("Corrected NMI value:", corrected_nmi_value)

print("SSIM Score:", ssim_score)
print("Correlation Coefficient:", correlation_coefficient[0][0])
print("Compare coefficient: ",compare1[0][0])
print("Compare coefficient: ",compare2[0][0])
print("Compare coefficient: ",compare3[0][0])


# Display original image pattern data
cv2_imshow(original_img)

# Display test image pattern data
cv2_imshow(test_img)

# Display difference image
cv2_imshow(diff_img)

plt.figure(figsize=[15,10]);
plt.subplot(2, 3, 1)
# plt.axis('off');
plt.imshow(compare1);
plt.title('diff_compare1')

plt.subplot(2, 3, 2)
# plt.axis('off');
plt.imshow(compare2);
plt.title('blend_compare2')

plt.subplot(2, 3, 3)
# plt.axis('off');
plt.imshow(compare3);
plt.title('checker_compare3')

plt.subplot(2, 3, 4)
# plt.axis('off');
plt.imshow(fabric_texture1);
plt.title('fabric1')

plt.subplot(2, 3, 5)
# plt.axis('off');
plt.imshow(fabric_texture2);
plt.title('fabric2')

cv2.waitKey(0)
cv2.destroyAllWindows()
plt.tight_layout()

# import cv2
# import numpy as np

# # Load original image and test image
# original_img = cv2.imread("/content/15.jpg", 0)
# test_img = cv2.imread("/content/64.jpg", 0)

# # Initialize SIFT detector
# sift = cv2.SIFT_create()

# # Find keypoints and descriptors in both images
# keypoints_original, descriptors_original = sift.detectAndCompute(original_img, None)
# keypoints_test, descriptors_test = sift.detectAndCompute(test_img, None)

# # Initialize matcher
# matcher = cv2.BFMatcher()

# # Match descriptors using KNN (k-nearest neighbors) algorithm
# matches = matcher.knnMatch(descriptors_original, descriptors_test, k=2)

# # Apply ratio test to filter good matches
# good_matches = []
# for m, n in matches:
#     if m.distance < 0.75 * n.distance:
#         good_matches.append(m)

# # Estimate homography using RANSAC
# MIN_MATCH_COUNT = 10
# if len(good_matches) > MIN_MATCH_COUNT:
#     src_pts = np.float32([keypoints_original[m.queryIdx].pt for m in good_matches]).reshape(-1, 1, 2)
#     dst_pts = np.float32([keypoints_test[m.trainIdx].pt for m in good_matches]).reshape(-1, 1, 2)
#     homography, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC, 5.0)
#     matches_mask = mask.ravel().tolist()
# else:
#     print("Not enough matches are found - %d/%d" % (len(good_matches), MIN_MATCH_COUNT))
#     matches_mask = None

# # Apply perspective transform to align images
# height, width = original_img.shape
# aligned_img = cv2.warpPerspective(test_img, homography, (width, height))

# # Display original image pattern data
# cv2_imshow( original_img)

# # Display test image pattern data
# cv2_imshow( test_img)

# # Display aligned test image
# cv2_imshow( aligned_img)

# cv2.waitKey(0)
# cv2.destroyAllWindows()





# from skimage.util import compare_images
# import os
# import cv2
# from skimage.metrics import structural_similarity
# from google.colab.patches import cv2_imshow

# def compare_images(folder_path, input_image):
#     """
#     Compares an input image with all images in a folder and finds the lowest correlation value.

#     Args:
#       folder_path: Path to the folder containing the images.
#       input_image: Path to the input image.

#     Returns:
#       The lowest correlation value found.
#     """

#     try:
#         # Load the input image
#         input_img = cv2.imread(input_image, 0)
#         input_height, input_width = input_img.shape

#         # Initialize the lowest correlation value
#         lowest_correlation = 1.0

#         # Iterate through all images in the folder
#         for filename in os.listdir(folder_path):
#             # Read the image
#             image = cv2.imread(os.path.join(folder_path, filename), 0)

#             # Resize the image to the shape of the input image
#             image_resized = cv2.resize(image, (input_width, input_height))

#             # Calculate the structural similarity index (SSIM) between the input image and the current image
#             similarity_index, _ = structural_similarity(input_img, image_resized, full=True, channel_axis=-1)
#             print('smlrty_score: ',similarity_index ,filename )


#             compare1 = compare_images(input_img,image_resized, method='diff', n_tiles=(8, 8))
#             compare2 = compare_images(input_img,image_resized, method='blend', n_tiles=(8, 8))
#             compare3 = compare_images(input_img,image_resized, method='checkerboard', n_tiles=(8, 8))
#             print("Compare coefficient: ",compare1[0][0])
#             print("Compare coefficient: ",compare2[0][0])
#             print("Compare coefficient: ",compare3[0][0])

#             # Update the lowest correlation value if necessary
#             if similarity_index < lowest_correlation:
#                 lowest_correlation = similarity_index

#                 # Display the images
#                 # cv2_imshow(image_resized)
#                 # # cv2_imshow(input_img)
#                 plt.figure(figsize=[15,10]);
#                 plt.subplot(2, 3, 1)
#                 # plt.axis('off');
#                 plt.imshow(image_resized);
#                 plt.title('dat_set_img')

#                 plt.subplot(2, 3, 2)
#                 # plt.axis('off');
#                 plt.imshow(input_img);
#                 plt.title('input_img')

#                 # plt.subplot(2, 3, 3)
#                 # # plt.axis('off');
#                 # plt.imshow(filename);
#                 # plt.title('ot_img')


#         # Return the lowest correlation value
#         return lowest_correlation
#     except Exception as e:
#         print("Error:", e)
#         return None

# # Example usage
# folder_path = "/content/abc"
# input_image = "/content/64.jpg"

# lowest_correlation = compare_images(folder_path, input_image)

# if lowest_correlation is not None:
#     print("Lowest correlation value:", lowest_correlation)



# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline
from skimage.metrics import structural_similarity
from google.colab.patches import cv2_imshow

def compare_images(folder_path, input_image):
    try:
        # Load the input image
        input_img = cv2.imread(input_image, 0)
        input_height, input_width = input_img.shape

        # Initialize the lowest correlation value
        lowest_correlation = 0.50

        # Iterate through all images in the folder
        for filename in os.listdir(folder_path):
            # Read the image
            image = cv2.imread(os.path.join(folder_path, filename), 0)

            # Resize the image to the shape of the input image
            image_resized = cv2.resize(image, (input_width, input_height))

            # Calculate the structural similarity index (SSIM) between the input image and the current image
            # similarity_index, _ = structural_similarity(input_img, image_resized, full=True, channel_axis=-1)
            # print('smlrty_score: ',similarity_index ,filename )

            # Compute SSIM between original and test images
            total_bins = int(image_resized.max() + 1)
            ssim_score, diff_img = ssim(image_resized, input_img, full=True, range=(0, total_bins),channel_axis=-1, multichannel=True)
            # print('ssim_score: ', ssim_score)


            # Update the lowest correlation value if necessary
            if ssim_score >= lowest_correlation:
                lowest_correlation = ssim_score
                print('ssim_score: ', ssim_score,filename)

                # Display the images
                # cv2_imshow(image_resized)
                # # cv2_imshow(input_img)
                plt.figure(figsize=[15,10]);
                plt.subplot(2, 3, 1)
                # plt.axis('off');
                plt.imshow(image_resized);
                plt.title('dat_set_img')

                plt.subplot(2, 3, 2)
                # plt.axis('off');
                plt.imshow(input_img);
                plt.title('input_img')

                # plt.subplot(2, 3, 3)
                # # plt.axis('off');
                # plt.imshow(filename);
                # plt.title('ot_img')


        # Return the lowest correlation value
        return lowest_correlation
    except Exception as e:
        print("Error:", e)
        return None

# Example usage
folder_path = "/content/drive/MyDrive/Fabrics data (File responses)/Fabric images  (File responses)"
input_image = "/64.jpg"

lowest_correlation = compare_images(folder_path, input_image)

if lowest_correlation is not None:
    print("Lowest correlation value:", lowest_correlation)





# Commented out IPython magic to ensure Python compatibility.
import os
import pandas as pd
import cv2
import matplotlib.pyplot as plt
# %matplotlib inline
from skimage.metrics import structural_similarity
from google.colab.patches import cv2_imshow

def compare_images(folder_path, input_image):
    try:
        # Load the input image
        input_img = cv2.imread(input_image, 0)
        input_height, input_width = input_img.shape

        # Initialize the lowest correlation value
        lowest_correlation = 0.50

        # Iterate through all images in the folder
        for filename in os.listdir(folder_path):
            # Read the image
            image = cv2.imread(os.path.join(folder_path, filename), 0)

            # Resize the image to the shape of the input image
            image_resized = cv2.resize(image, (input_width, input_height))

            # Calculate the structural similarity index (SSIM) between the input image and the current image
            similarity_index, _ = structural_similarity(input_img, image_resized, full=True, channel_axis=-1)
            # print('smlrty_score: ',similarity_index ,filename )


            # Update the lowest correlation value if necessary
            if similarity_index >= lowest_correlation:
                lowest_correlation = similarity_index
                print("correlation value:", lowest_correlation,'smlrty_score: ',similarity_index ,filename )

                # Display the images
                # cv2_imshow(image_resized)
                # # cv2_imshow(input_img)
                plt.figure(figsize=[15,10]);
                plt.subplot(2, 3, 1)
                # plt.axis('off');
                plt.imshow(image_resized);
                plt.title('dat_set_img')

                plt.subplot(2, 3, 2)
                # plt.axis('off');
                plt.imshow(input_img);
                plt.title('input_img')

                # plt.subplot(2, 3, 3)
                # # plt.axis('off');
                # plt.imshow(filename);
                # plt.title('ot_img')

            else:
              pass

        # Return the lowest correlation value
        return lowest_correlation
    except Exception as e :
        print("Error:",e )
        return None

# Example usage
folder_path = "/content/drive/MyDrive/Fabrics data (File responses)/Fabric images  (File responses)"
input_image = "/64.jpg"

lowest_correlation = compare_images(folder_path, input_image)

# if lowest_correlation is not None:
#     print("correlation value:", lowest_correlation)

